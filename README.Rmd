---
title: "mega-post-bcf-exploratory-snakeflows"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I am just starting to put this together.  The basic idea is that when exploring
NGS data that comes to you in a BCF file, there are lots of ways you might want to
tweak it in the process.  For example:

- Use different BCF files
- Use different subsets of individuals
- Use different filtering criteria
- Thin the number of markers to a different level
- Thin the number of markers to the same level, but do
   different randomly sampled replicates of that.

On top of that, once you start analyzing those data you will want to try different
things:

- Different values of K with ngsAdmix
- Different replicate runs of ngsAdmix
- Different parameter settings of other packages
- Some you will want to break over chromosomes to parallelize
- Others, you can't break over chromosomes, etc.

So, these are all just the things I am thinking about as I start noodling around with
a few things here.  

I am thinking about a wildcarding/directory structure that looks something like this
for the basic BCF maneuvers:
```python
"bcf_{bcf_file}/samp-sub_{sample_subset}/filt_{filter_crit}/tf_{thinning_factor}/tf-seed_{thin_factor_seed}/"
```

But, now that I think about it, I'd say the thinning factors should be part of the
filtering criteria.  So, instead, the basic structure would be:
```python
bcf_{bcf_file}/thin_{thin_int}_{thin_start}/samp-sub_{sample_subset}/filt_{filter_crit}/
```

My idea for these wildcards is, then:

* `{bcf_file}` is the tag given in a TSV file to a path to a BCF
* `{thin_int}` and `{thin_start}`  These are there for doing simple deterministic
thinning of the file, starting at variant `thin_start` and then taking every
`thin_int`-th marker after that.  If either of them is 0, there is no thinning done.
* `{sample_subset}` is the name given to a subset of samples in the BCF file.  The actual
samples themselves are given in a file, typically 
`config/sample_subsets/{sample_subset}.txt`, that can be read easily by bcftools
or vcftools.  The exact location of the `sample_subsets` directory will be given 
in config.yaml.
* `{filter_crit}` will match the ID line in a TSV file that has other columns,
bcftools and vcftools, in which the relevant filtering option values are given.
Replicates of thinning can be given with IDs like "FILT-1", "FILT-2", and so forth.

Well, that is enough theory for now.  We should probably start building some things
up.

Hey! For parallelizing over genomic regions I am just going to have a single
`scaffold_groups.tsv` file that includes both chromosomes and scaffolds.  i.e. some
scaffold groups might include just a single chromosome.

News Flash!  The path to that file is going to be a column in the `bcfs.tsv`, because
we might have different BCFs that are mapped to different genomes, etc.  Also, there 
should be a column that gives the path of the indexed genome, for some ANGSD applications
I do believe.

## Required files

* `scaffold_groups.tsv`: two columns, `id` and `chrom`, **in that order**.
Each row is a chromosome from the reference genome, in the order that
they appear in the genome.  

## Development

### Wildcards in play at the moment

* `bcf_file`
* `scaff_grp`
* `sample_subset`
* `bcftools_opts`



### Test data set

We could use a decent test data set for this. I have 480-something chinook, filtered
to maf 0.01 that is about 31 Gb, and has around 14 million variants in it. If we sampled
at a rate of 0.0068 we would end up with about 100,000 markers, but that would still
be about 210 Mb.  So, let's cut it down to 10,000 markers and around 22 Mb.

```sh
mamba create -n vcflib-1.0.3 -c bioconda vcflib
conda activate vcflib-1.0.3
module load bio/bcftools

bcftools view pass-maf-0.01.bcf | vcfrandomsample -r 0.00068 | bcftools view -Ob > /tmp/test.bcf

```
Ha!  That is abominably slow!  Granted it is streaming through the whole file to pick out
less than 1 in a 1000.  

So, while waiting for that, I try it a different way:
```sh
# print every 1000-th marker to a file:
 bcftools query -f '%CHROM\t%POS\n' pass-maf-0.01.bcf | awk 'NR%1000==0' > /tmp/get-these.txt
 # that should get us about 14000 markers.
 
# then, I will take only half of those (get us down to about 7000 markers)
awk 'NR%2==0' get-these.txt > markers.txt

# and then we can access them via the index with the -R option
bcftools view -Ob  -R /tmp/markers.txt  pass-maf-0.01.bcf  > ~/test.bcf
```
That got done by the time the vcflib streaming approach was still on marker
1000.  OK...,let's here it for log2N binary search of indexed BCF files.

The resulting BCF file is 19 Mb. Perhaps still too big for a real .test data set, but I can
start playing with it now on my laptop, and on SEDNA.

### scaff groups for it

I am going to make these from the chromosomes and scaff groups.
```{r}
library(tidyverse)
```
```{r, eval=FALSE}
chrom <- read_tsv("~/Documents/projects/yukon-chinookomes/chromosomes.tsv") %>%
  mutate(id = as.character(1:n(), .before = chrom)
) %>%
  rename(stop = num_bases)
sg <- read_tsv("~/Documents/projects/yukon-chinookomes/scaffold_groups.tsv") %>%
  select(id, chrom, len) %>%
  rename(stop = len)

both <- bind_rows(chrom, sg) %>%
  mutate(
    id2 = sprintf("scaff_grp_%04d", as.integer(factor(id, levels = unique(id))))
  ) %>%
  mutate(id = id2) %>%
  mutate(start = 1) %>%
  select(id, chrom, start, stop)

write_tsv(both, ".test/bcf/test.bcf.scaff_groups.tsv")
```

### beagle3_glikes





